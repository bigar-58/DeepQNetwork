{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHbe9XbSouMJ"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIe8pyCN3-Ls"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import cv2\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mme8X2eo33pj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlCYMKQq35Wp"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTtxKgqv4Seh"
      },
      "source": [
        "#Training Stable Baselines 3 DQN on Breakout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ve3N-dm4DXB"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class CustomCallback(BaseCallback):\n",
        "    def __init__(self, save_path, screenshot_freq, smoothing_window=50):\n",
        "        super(CustomCallback, self).__init__()\n",
        "        self.save_path = save_path\n",
        "        self.screenshot_freq = screenshot_freq\n",
        "        self.all_episode_rewards = []\n",
        "        self.losses = []\n",
        "        self.q_values = []\n",
        "        self.smoothing_window = smoothing_window\n",
        "\n",
        "    def _on_step(self):\n",
        "        info = self.locals.get(\"infos\", [{}])[-1]\n",
        "        done = self.locals.get(\"dones\", [False])[-1]\n",
        "\n",
        "        # Store loss and Q-values\n",
        "        self.losses.append(info.get('loss', 0))\n",
        "        self.q_values.append(info.get('mean_q_value', 0))\n",
        "\n",
        "        # Check if an episode has finished\n",
        "        if done:\n",
        "            episode_reward = info.get(\"episode\", {}).get(\"r\", 0)\n",
        "            self.all_episode_rewards.append(episode_reward)\n",
        "\n",
        "        # Take a screenshot and print scores every 'screenshot_freq' steps\n",
        "        if self.num_timesteps % self.screenshot_freq == 0:\n",
        "            img = self.training_env.render(mode='rgb_array')\n",
        "            cv2.imwrite(f\"{self.save_path}/screenshot_{self.num_timesteps}.png\", img)\n",
        "\n",
        "            # Calculate and print current max and average score\n",
        "            if self.all_episode_rewards:\n",
        "                max_score = max(self.all_episode_rewards)\n",
        "                avg_score = sum(self.all_episode_rewards) / len(self.all_episode_rewards)\n",
        "                print(f\"Step: {self.num_timesteps}, Max Score: {max_score}, Average Score: {avg_score}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def smooth(self, data):\n",
        "        # Apply a moving average for smoothing\n",
        "        return np.convolve(data, np.ones(self.smoothing_window)/self.smoothing_window, mode='valid')\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        # Plotting the episode rewards, loss, and Q-values after training\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Rewards\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(self.smooth(self.all_episode_rewards), label='Smoothed Episode Rewards')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Smoothed Rewards per Episode')\n",
        "\n",
        "        # Loss\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(self.smooth(self.losses), label='Smoothed Loss')\n",
        "        plt.xlabel('Steps')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Smoothed Loss over Time')\n",
        "\n",
        "        # Q-Values\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(self.smooth(self.q_values), label='Smoothed Q-Values')\n",
        "        plt.xlabel('Steps')\n",
        "        plt.ylabel('Q-Values')\n",
        "        plt.title('Smoothed Q-Values over Time')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"{self.save_path}/training_metrics.png\")\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2FvGxe54Hcc"
      },
      "outputs": [],
      "source": [
        "model_path_cnn = 'path_to_CNN model'\n",
        "save_path = 'path to save screenshots to'\n",
        "callback = CustomCallback(save_path=save_path, screenshot_freq=50000)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env_cnn = make_atari_env('BreakoutNoFrameskip-v4', n_envs=1, seed=0)\n",
        "env_cnn = VecFrameStack(env_cnn, n_stack=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn3A30lv4RS6"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "    loaded_model = DQN.load(model_path_cnn, env=env_cnn)\n",
        "    loaded_model.learn(total_timesteps=500000, callback=callback)\n",
        "    loaded_model.save(model_path_cnn)\n",
        "\n",
        "    if i>0 and i % 2 == 0: callback.plot_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2csqYOL_4bxP"
      },
      "source": [
        "#Evaluate DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgdK8LLc5K9c"
      },
      "source": [
        "#Experiments on Gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCrmigNo5ru6"
      },
      "source": [
        "Switch model by loading in different gamma value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOyM2cQd5JHT"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn, env=env_cnn, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9s_WiOE5gv7"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn, env=env_cnn, gamma=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jkmXOAJ5hGU"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn, env=env_cnn, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5lDaFuv5nBt"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy and get individual episode rewards\n",
        "episode_rewards, episode_lengths = evaluate_policy(loaded_model, env_cnn, n_eval_episodes=10, render=False, return_episode_rewards=True)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "print(np.max(episode_rewards))\n",
        "print(f\"Environment: Breakout, Mean Reward: {mean_reward}, Std: {std_reward}\")\n",
        "\n",
        "env_cnn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQOQGEUQ5NYH"
      },
      "source": [
        "#Experiments on Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGyXI61B405L"
      },
      "source": [
        "Breakout Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFvbJD8d4iFn"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn, env=env_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5nj72v64mGM"
      },
      "outputs": [],
      "source": [
        "# Create and wrap the environment\n",
        "env_cnn = make_atari_env('BreakoutNoFrameskip-v4', n_envs=1)\n",
        "env_cnn = VecFrameStack(env_cnn, n_stack=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrPHvZQP4nk_"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy and get individual episode rewards\n",
        "episode_rewards, episode_lengths = evaluate_policy(loaded_model, env_cnn, n_eval_episodes=10, render=False, return_episode_rewards=True)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "print(np.max(episode_rewards))\n",
        "print(f\"Environment: Breakout, Mean Reward: {mean_reward}, Std: {std_reward}\")\n",
        "\n",
        "env_cnn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il1U_U1K432Z"
      },
      "source": [
        "Pong Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGtxMFJA4sfY"
      },
      "outputs": [],
      "source": [
        "env_pong = make_atari_env('PongNoFrameskip-v4', n_envs=1, seed=0)\n",
        "env_pong = VecFrameStack(env_pong, n_stack=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrR5-NQe4tEc"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo3itli14uVI"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy and get individual episode rewards\n",
        "episode_rewards, episode_lengths = evaluate_policy(loaded_model, env_pong, n_eval_episodes=10, render=False, return_episode_rewards=True)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "\n",
        "print(f\"Environment: Pong, Mean Reward: {mean_reward}, Std: {std_reward}\")\n",
        "\n",
        "env_pong.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkAinHWC46MA"
      },
      "source": [
        "Space Invaders Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6eo9xtO4wzR"
      },
      "outputs": [],
      "source": [
        "env_SI = make_atari_env('SpaceInvadersNoFrameskip-v4', n_envs=1, seed=0)\n",
        "env_SI = VecFrameStack(env_SI, n_stack=3) #Paper mentions this, but to see lasers, we need to lower to stack 3 frames instead of 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umJKyUbP4xI6"
      },
      "outputs": [],
      "source": [
        "loaded_model = DQN.load(model_path_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IcWCZam4yj6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy and get individual episode rewards\n",
        "episode_rewards, episode_lengths = evaluate_policy(loaded_model, env_SI, n_eval_episodes=10, render=False, return_episode_rewards=True)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "\n",
        "print(f\"Environment: Space Invaders, Mean Reward: {mean_reward}, Std: {std_reward}\")\n",
        "\n",
        "env_SI.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
